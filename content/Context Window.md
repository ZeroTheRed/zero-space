>[!Definition]
>The amount of tokens that an [[LLMs|LLM]] can immediately parse as input for generating a response[^1] 

The length of a *context window* is measured in the number of *tokens*. **Larger context windows improve LLM performance and reliability** 

[^1]: https://www.hopsworks.ai/dictionary/context-window-for-llms