>[!Definition]
>**RAG** is the method **LLMs** use to **retrieve documents and information** to **augment and support response generation**[^1]

^b2a5f7

RAG uses **vector databases** to store up-to-date information that's retrieved using **semantic searches** and **added to the prompt's context window**

1. **Locate the knowledge sources** - Information is *broken down into chunks* by a data-processing service. Then, it's *encoded into a vector representation* using an [[Embedding Model]]. The vectors are stored in a [[Vector Database]] for future retrieval
2. **The system parses the user prompt** - Once the prompt is given to the LLM, the system encodes it into a *vector similar to that of vectors generated by an embedding model in data ingestion*. There, the vector's contents are *semantically matched* with the most relevant data in the *vector database obtained from step 1* and the matches are injected into the prompt's *context*. Finally, the *augmented prompt* is sent to the LLM

RAG is useful in mitigating *hallucination* but it cannot eliminate it entirely. How is it helpful?
It *provides additional context and information* that the LLM has previously been unaware of

[^1]: https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/